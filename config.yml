debug: False
max_examples_per_dataset: -1

path_server: /home/johannesg/Projects/llama.cpp/build/bin/llama-server
path_quantize: /home/johannesg/Projects/llama.cpp/build/bin/llama-quantize
path_model: /opt/models/{name}-{quantization}.gguf
path_imatrix: /opt/models/{name}.imatrix

datasets:
  - gpqa_main
  - gsm8k_test
  - mmlu_test
  - mmlu-pro_test
prompt_types:
  - instant
  - normal

quantizations:
  - f16
  # - q8_0
  # - q6_k
  # - q5_k_m
  # - q5_k_s
  # - q4_k_m
  # - q4_k_s
  # - q3_k_l
  # - q3_k_m
  # - q3_k_s
  # - q2_k
  # - iq4_nl
  # - iq4_xs
  # - iq3_m
  # - iq3_s
  # - iq3_xs
  # - iq3_xxs
  # - iq2_m
  # - iq2_s
  # - iq2_xs
  # - iq2_xxs
  # - iq1_m
  # - iq1_s
parallel: 8
gpus_per_job: 1

models:
  - name: gemma_2_it-9b
    parallel: 2
  - name: gemma_3_it-1b
  - name: gemma_3_it-4b
  # - name: gemma_3_it-12b
  #   gpus_per_job: 2
  - name: glm_4_chat-9b
  - name: llama_3.1_instruct-8b
  - name: llama_3.2_instruct-1b
  - name: llama_3.2_instruct-3b
  - name: llama_4_scout_17b_16e_instruct-108b
    quantizations: [q3_k_h]
    gpus_per_job: 3
  - name: ministral_instruct_2410-8b
  - name: mistral_nemo_instruct_2407-12b
    gpus_per_job: 2
  - name: mistral_small_3.1_instruct_2503-24b
    gpus_per_job: 3
  - name: phi_4-15b
    gpus_per_job: 2
  - name: phi_4_mini_instruct-4b
  - name: qwen_2.5_instruct_1m-7b
  # - name: qwq-32b
  #   # gpus_per_job: 4
  #   quantizations: [q8_0]
  #   gpus_per_job: 2
  - name: stablelm_2_chat-2b
