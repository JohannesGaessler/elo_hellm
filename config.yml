server_binary: /home/johannesg/Projects/llama.cpp/build/bin/llama-server
filepath: /opt/models/{name}-{quantization}.gguf
quantizations:
  - f16

models:
  - name: llama_3.2_instruct-1b
  - name: llama_3.2_instruct-3b
